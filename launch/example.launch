<!--
Copyright (c) 2017, Adam Allevato
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

1. Redistributions of source code must retain the above copyright notice,
   this list of conditions and the following disclaimer.

2. Redistributions in binary form must reproduce the above copyright
   notice, this list of conditions and the following disclaimer in the
   documentation and/or other materials provided with the distribution.

3. Neither the name of the copyright holder nor the names of its
   contributors may be used to endorse or promote products derived from
   this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS
IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR
CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-->

<!-- This launch file is an example of how to use ORP in your own application,
     using one of the built-in classifiers.

     The example uses an Intel Realsense R200, and attempts to find small
     objects on a planar surface. It detects those objects by their color, with
     names such as "obj_blue", "obj_orange", etc.

     You can easily modify this example to work with
     OpenNI2 (Asus Xtion) or Orbbec Astra cameras.
-->
<launch>
  <!-- Before calling orp.launch, your list of items should be loaded onto the
       ROS parameter server. The following line does just that. -->
  <rosparam command="load" param="/orp/items" file="$(find orp)/data/example_database.yaml" />

  <!-- orp.launch loads some default configuration parameters, such as settings
       for the segmentation server. If you want to override these, you should
       add a <rosparam load> tag somewhere after the orp.launch call. -->
  <include file="$(find orp)/launch/orp.launch">
    <!-- You can enable different cameras here, or not enable any of them if
         you are already running a camera from a different method (such as
         one of your own launch files). There are a few supported cameras,
         read orp.launch for more details. -->
    <arg name="realsense_4xx" value="true" />

    <!-- You can toggle different classifiers with flags. Two common ones you
         can try out are "hue" and "rgb" classifiers. You can run multiple
         classifiers at once. You can also not enable any classifiers here
         if you want to run your own classifiers. -->
    <arg name="hue" value="true" />

    <!-- By default, ORP does not start the recognition update loop immediately
         to avoid hogging CPU (since depth processing can be very expensive).
         Setting autostart to true overrides this behavior. -->
    <arg name="autostart" value="true" />

    <!-- Your clipping parameters will be specified relative to this frame.
         Every detected object will also be specified using PoseStamped
         messages relative to this frame. Ideally you should pick a frame
         that is easy to understand for motion planning and/or clipping.
         For example, using your camera_link is probably not the best frame
         available because it may be at an odd angle based on your camera
         setup. Consider using base_link, world, or whatever frame you have
         that will be oriented in an intuitive way. -->
    <arg name="recognition_frame" value="/camera_link" />

    <!-- Once you run the example, you will probably tune the segmentation
         parameters such as clipping bounds, voxel size, min/max cluster size,
         etc. Once you have settings you like, you can save them using the
         terminal command `rosparam dump orp_settings.yaml /orp`. Once you've
         done that, you can pass the path to that file to this argument so you
         don't have to re-tune every time. -->
    <arg name="config_file" value="$(find orp)/cfg/default.yaml" />

    <!-- Load up some sensible RViz defaults that are good for visualizing
         ORP's output. The default for this parameter is false, so if you
         don't want RViz to come up you can remove this argument
         completely. -->
    <arg name="rviz" value="true" />
  </include>

  <!-- additional launch files could be added here to finish the vision stack.
       It may be necessary to define one or more TF static_transform_publisher
       nodes to define the transformations between the camera and the world.-->
</launch>
